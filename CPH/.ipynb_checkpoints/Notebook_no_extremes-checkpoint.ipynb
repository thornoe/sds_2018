{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip3 install tqdm\n",
    "!pip3 install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoPandas\n",
    "# first run in terminal: conda install geopandas\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from shapely.geometry import Point\n",
    "import requests, json, random, time#, #tqdm, warnings\n",
    "\n",
    "#import geopy.geocoders  # GeoPy - see https://pypi.org/project/geopy/\n",
    "#from geopy.geocoders import Nominatim # retrieve coordinates from addresses etc.\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS**: To skip the scraping, cleaning and merging part just jump to section 2 or 3 and read the data from github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Scraping function for Bolighed.dk\n",
    "This is a function to scrape data on supply prices for real estate apartments from Bolighed.dk.\n",
    "\n",
    "**OBS**: To skip this scraping part (about 18 minutes) just go to the next section and read the data from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we test if we can get some data from Bolighed \n",
    "url = 'https://bolighed.dk/api/external/market/propertyforsale/?limit=40&offset=0&view=list&type__in=300&ordering=mtid'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.ok:  # response.ok is True if status code is 200\n",
    "    r = response.json()\n",
    "else:\n",
    "    print('error')\n",
    "    \n",
    "listings = r['count'] # = 7855 opslag\n",
    "\n",
    "# Collect all links from search on Bolighed by changing the page-parameter\n",
    "links = []\n",
    "\n",
    "for offset in range(0,listings+40,40):\n",
    "    url = 'https://bolighed.dk/api/external/market/propertyforsale/?limit=40&offset={o}&view=list&type__in=300&ordering=mtid'.format(o = offset)\n",
    "    links.append(url)\n",
    "    \n",
    "# This code is for scraping all pages off the search criteria for owner appartments. NB!! Only run this once. \n",
    "done = set()\n",
    "data = []\n",
    "\n",
    "for url in tqdm.tqdm(links):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.ok:\n",
    "        r = response.json()\n",
    "    else:\n",
    "        print('error')\n",
    "\n",
    "    data += r['results']\n",
    "    time.sleep(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scraped data\n",
    "data.to_csv('C:/Users/thorn/OneDrive/Dokumenter/GitHub/sds_2018/CPH/Data/raw_data.csv', index=False)  # Save scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Function to structure raw data output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datastructuring(data, timeout):\n",
    "    # import packages\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import time, tqdm\n",
    "    import geopy.geocoders  # GeoPy - see https://pypi.org/project/geopy/\n",
    "    from geopy.geocoders import Nominatim # retrieve coordinates from addresses etc.\n",
    "    geopy.geocoders.options.default_user_agent = 'my_app/1'\n",
    "    geopy.geocoders.options.default_timeout = timeout\n",
    "\n",
    "    # read data\n",
    "    raw_data = pd.DataFrame(data)\n",
    "\n",
    "    # Keep the columns we want to keep and name them\n",
    "    sorted_data = raw_data.iloc[:,[0,2,3,4,7,8,12,14,15,16,18,21]]\n",
    "    sorted_data.columns = ['Address', 'Rooms', 'Area', 'Land_area', 'Owner_expense',\n",
    "                           'Energy_mark', 'Price', 'Days_on_market',\n",
    "                           'Zip_code', 'Town', 'Price_development', 'Sqm_price']\n",
    "\n",
    "    # Unpack all latitudes and longitudes and add to dataframe\n",
    "    longitude = []\n",
    "    latitude = []\n",
    "    raw_data['geometry']\n",
    "\n",
    "    for i, row in raw_data['geometry'].iteritems():\n",
    "        if type(row) == str:\n",
    "            coordinates = row.split('[', 1)[1]  # after '['\n",
    "            longitude.append(float(coordinates.split(',', 1)[0]))  # before ','\n",
    "            lat = coordinates.split(', ', 1)[1]  # after ', '\n",
    "            latitude.append(float(lat.split(']', 1)[0]))  # before ']'\n",
    "        else:\n",
    "            longitude.append(None)\n",
    "            latitude.append(None)\n",
    "    # Add latitude and longitude to sorted dataframe.\n",
    "    sorted_data.insert(loc=0, column='Latitude', value=latitude)\n",
    "    sorted_data.insert(loc=0, column='Longitude', value=longitude)\n",
    "\n",
    "    # Convert zip_code from str to int\n",
    "    sorted_data['Zip_code'] = sorted_data['Zip_code'].astype(int, inplace=True)\n",
    "    # Sort only zipcodes from Copenhagen\n",
    "    cph = sorted_data[(sorted_data.Zip_code < 3000)].copy()\n",
    "    cph.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Make a column of indexes for energy mark\n",
    "    energysaving = []\n",
    "    for i, row in cph['Energy_mark'].iteritems():\n",
    "        if row == 'G':\n",
    "            energysaving.append(0)\n",
    "        elif row == 'F':\n",
    "            energysaving.append(1)\n",
    "        elif row == 'E':\n",
    "            energysaving.append(2)\n",
    "        elif row == 'D':\n",
    "            energysaving.append(3)\n",
    "        elif row == 'C':\n",
    "            energysaving.append(4)\n",
    "        elif row == 'B':\n",
    "            energysaving.append(5)\n",
    "        elif row == 'A':\n",
    "            energysaving.append(6)\n",
    "        elif row == 'A10':\n",
    "            energysaving.append(7)\n",
    "        elif row == 'A15':\n",
    "            energysaving.append(8)\n",
    "        elif row == 'A20':\n",
    "            energysaving.append(9)\n",
    "        else:\n",
    "            energysaving.append(None)\n",
    "    cph.insert(loc=0, column='Energy_saving', value=energysaving)\n",
    "\n",
    "    # Set all missing values with energy_saving to mean.\n",
    "    cph.Energy_saving.fillna(5.0, inplace=True)\n",
    "\n",
    "    # Create 'floor' variable\n",
    "    floor = []\n",
    "    for i, row in cph['Address'].iteritems():\n",
    "        if ',' in row:\n",
    "            sec_part = row.split(', ', 1)[1] # split once, keep 2nd part\n",
    "            if sec_part[:2].isdigit():  # 399 with two or more digits (unindentified floor, 362 >= '20')\n",
    "                floor_int = int(sec_part[0])  # assume 1st digit indicates floor\n",
    "                floor.append(floor_int)\n",
    "            elif sec_part[0].isdigit():\n",
    "                floor_int = int(sec_part[0])\n",
    "                floor.append(floor_int)\n",
    "            else:\n",
    "                floor.append(int(0))\n",
    "        else:\n",
    "            floor.append(int(0))\n",
    "    cph.insert(loc=0, column='Floor', value=floor)\n",
    "\n",
    "    ##########################################################################\n",
    "    #           Code missing latitudes and longitudes using GeoPy            #\n",
    "    ##########################################################################\n",
    "    address_simple = []\n",
    "    for i, row in cph['Address'].iteritems():\n",
    "        if 'George Marshalls Vej' in row:\n",
    "            address_simple.append('Fiskerihavnsgade 8')\n",
    "        elif 'Amerika Plads' in row:\n",
    "            address_simple.append(row.replace('Plads', 'Pl.'))\n",
    "        elif 'HUSBÅD' in row:\n",
    "            address_simple.append(row.split(' - HUSBÅD', 1)[0])  # Keep first part\n",
    "        else:\n",
    "            address_simple.append(row.split(',', 1)[0]) # split once, keep 1st part\n",
    "    cph.insert(loc=0, column='Address_simple', value=address_simple)\n",
    "\n",
    "    town_simple = []\n",
    "    for i, row in cph['Town'].iteritems():\n",
    "        if 'København' in row:\n",
    "            town_simple.append('København')  # Keep 'København' only\n",
    "        elif 'Nordhavn' in row:\n",
    "            town_simple.append('København')  # Keep 'København' only\n",
    "        else:\n",
    "            town_simple.append(row)\n",
    "    cph.insert(loc=0, column='Town_simple', value=town_simple)\n",
    "    cph['Full_address'] = cph['Address_simple'].map(str) + ', ' + cph['Zip_code'].map(str) + ' ' + cph['Town_simple']\n",
    "\n",
    "    # Thoose with missing latitude and longitude from scrape\n",
    "    cph['Missing'] = cph['Longitude'].isnull()\n",
    "    cph['Full_add'] = cph['Full_address']*cph['Missing']\n",
    "\n",
    "    # Retrieve coordinates from column of addresses\n",
    "    geolocator = Nominatim()\n",
    "    # geolocator.headers  # check header\n",
    "    # geolocator.timeout  # check time_out\n",
    "    lati = []\n",
    "    longi = []\n",
    "\n",
    "    for row in tqdm.tqdm(cph['Full_add']):\n",
    "        row_string = str(row)\n",
    "        if len(row_string) > 0:\n",
    "            location = geolocator.geocode(row_string)\n",
    "            if isinstance(location, geopy.location.Location):\n",
    "                lati.append(float(location.latitude))\n",
    "                longi.append(float(location.longitude))\n",
    "            else:\n",
    "                print('Not found: ',row_string)\n",
    "                lati.append(None)\n",
    "                longi.append(None)\n",
    "        else:\n",
    "            lati.append(None)\n",
    "            longi.append(None)\n",
    "    cph.insert(loc=0, column='Lati', value=lati)\n",
    "    cph.insert(loc=0, column='Longi', value=longi)\n",
    "    cph['Latitude'] = cph['Latitude'].fillna(cph['Lati'])\n",
    "    cph['Longitude'] = cph['Longitude'].fillna(cph['Longi'])\n",
    "    cph.isnull().sum()\n",
    "    # Drop columns\n",
    "    # cph = cph.drop(['Longi', 'Lati', 'Town_simple', 'Address_simple',\n",
    "    #     'Energy_mark', 'Town', 'Full_address', 'Missing', 'Full_add'], axis=1)\n",
    "\n",
    "    ##########################################################################\n",
    "    #                      Append municipality onto dataset                  #\n",
    "    ##########################################################################\n",
    "    # Get data from Statistics Denmark with zip_code:\n",
    "    # Get zip codes and municipalities\n",
    "    url_post = 'https://www.dst.dk/ext/4393839853/0/kundecenter/Tabel-Postnumre-kommuner-og-regioner--xlsx'\n",
    "    df_muni = pd.read_excel(url_post)\n",
    "    df2_muni = df_muni[4:]\n",
    "    df2_muni.rename(columns={'Postnumre, kommuner og regioner, 1.1.2016':'Zip','Unnamed: 1':'Municipality','Unnamed: 2':'Region'}, inplace=True)\n",
    "\n",
    "    # Split data: we want to seperate zip code and village as well as municipality number and municipality\n",
    "    zip_split = pd.DataFrame(df2_muni.Zip.str.split(' ',1).tolist(),\n",
    "                                       columns = ['Zip','Village'])\n",
    "\n",
    "    mun_split = pd.DataFrame(df2_muni.Municipality.str.split(' ',1).tolist(),\n",
    "                                       columns = ['Mun. no.','Municipality'])\n",
    "\n",
    "    merge = pd.concat([zip_split, mun_split], axis=1, sort=False)\n",
    "\n",
    "    #Construct new variable that only contain municpalities with zip code below 3000\n",
    "    mun_zip = merge[['Zip','Municipality']]\n",
    "    mun_zip['Int zip'] = mun_zip['Zip'].astype(int)\n",
    "    our_sample = mun_zip[(mun_zip['Int zip'] < 3000)]\n",
    "    # Drop string zip_code column:\n",
    "    our_sample.drop('Zip', axis=1 ,inplace=True)\n",
    "    # Give common name to this datasets zip code and our:\n",
    "    our_sample.rename(columns={'Int zip': 'Zip_code'}, inplace=True)\n",
    "    # Keep last duplicate of kommunes\n",
    "    our_sample.drop_duplicates(keep='last', subset='Zip_code', inplace=True)\n",
    "    # Now merge datasets on zip code:\n",
    "    cph_merged= pd.merge(cph, our_sample, on='Zip_code', how='left')\n",
    "\n",
    "\n",
    "    ##########################################################################\n",
    "    #                 Sort columns in order which makes sence                #\n",
    "    ##########################################################################\n",
    "    cph_kom =cph_merged.reindex(columns=['Address', 'Zip_code', 'Municipality',\n",
    "        'Latitude', 'Longitude', 'Floor', 'Rooms', 'Area', 'Land_area',\n",
    "        'Price', 'Sqm_price', 'Price_development', 'Owner_expense',\n",
    "        'Energy_saving', 'Days_on_market'])\n",
    "\n",
    "\n",
    "    ##########################################################################\n",
    "    #                   Other measures of (total) expenses                   #\n",
    "    ##########################################################################\n",
    "    # Create log variable of square meter price\n",
    "    cph_kom.insert(loc=11, column='log_sqm_price', value=np.log(cph_kom.Sqm_price))\n",
    "\n",
    "    # Owner expenses pr. square meter\n",
    "    cph_kom.insert(loc=14, column='Sqm_owner_expenses', value=np.divide(cph_kom.Owner_expense, cph_kom.Area))\n",
    "\n",
    "    # Standard-Finance (total yearly expenses)\n",
    "    yearly_expenses = []\n",
    "    first_year_expenses = []\n",
    "    for houseprice, ownerexp in zip(cph_kom.Price, cph_kom.Owner_expense):\n",
    "        \"\"\"Vi antager at køberne selvfinansiere de 20 % af købssummen således:\"\"\"\n",
    "        remaining = houseprice % 5000  # rest, når der deles med 5.000\n",
    "        price = houseprice*0.05\n",
    "        if remaining < 2500:\n",
    "            price = int(price / 5000) * 5000  # runder ned\n",
    "        else:\n",
    "            price = int((price + 5000) / 5000) * 5000  # runder op\n",
    "        Cashticket = max(price,25000)  # Kontant udbetaling på 5% af den kontante købesum oprundet til nærmeste kr. 5.000 dog minimum kr. 25.000.\n",
    "        Mortgage = (houseprice*0.8)*0.03  # Der lånes 80% i realkreditinstitutet til en ÅOP på 3% efter skat.\n",
    "        Bankloan = (houseprice*0.2 - Cashticket)*0.066  # De resterende ca. 15% lånes i banken til en ÅOP på 6.6% efter skat banklånet\n",
    "        yearly_expenses.append(12*ownerexp + Bankloan + Mortgage)\n",
    "        first_year_expenses.append(12*ownerexp + Bankloan + Mortgage + Cashticket)\n",
    "    cph_kom.insert(loc=15, column='Yearly_expenses', value=yearly_expenses)\n",
    "    cph_kom.insert(loc=16, column='First_year_expenses', value=first_year_expenses)\n",
    "    sqm_yearly_exp = (cph_kom.Yearly_expenses / cph_kom.Area)\n",
    "    cph_kom.insert(loc=17, column='Sqm_yearly_expenses', value=(sqm_yearly_exp))\n",
    "    cph_kom.insert(loc=18, column='log_sqm_yearly_exp', value=np.log(sqm_yearly_exp))\n",
    "\n",
    "    print(cph_kom.isnull().sum())\n",
    "\n",
    "    return cph_kom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv to skip the scraping part above\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/thornoe/sds_2018/master/CPH/Data/raw_data.csv')\n",
    "\n",
    "# Create data set from structuring function: \n",
    "CPH = datastructuring(data, 15)\n",
    "\n",
    "# Compare columns:\n",
    "# raw=pd.DataFrame(data)\n",
    "# print('First we had a raw data set with ' + str(len(raw)) + ' observations.') #7933 observations\n",
    "# print('Now we have a sorted data set with ' + str(len(CPH)) + ' observations.')    #3924 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get an overview of types and dataframe:\n",
    "print(CPH.dtypes)\n",
    "CPH.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Calculation of distance to school, metro and jail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code is structured as followed\n",
    "- Calculating distances to a top 20% school\n",
    "    - Scraping list of best all schools \n",
    "    - Use *`geocode`* to get coordinates\n",
    "    - Calculate distances from each apartment\n",
    "    - Construct list of minimum distance to top school\n",
    "- Calculating distances to a metro\n",
    "- Calculating distances to a jail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## School data\n",
    "In the first part of this code, we retrieve data on municipalities and which zip code that belong to which municipality.\n",
    "\n",
    "Because many of the columns are intertwined, we need to split and merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get zip codes and municipalities from DST\n",
    "url_post = 'https://www.dst.dk/ext/4393839853/0/kundecenter/Tabel-Postnumre-kommuner-og-regioner--xlsx'\n",
    "df_muni = pd.read_excel(url_post)\n",
    "df2_muni = df_muni[4:]\n",
    "df2_muni.columns = ['Zip', 'Municipality','Region']\n",
    "\n",
    "\n",
    "# Split data: we want to seperate zip code and village as well as municipality number and municipality\n",
    "zip_split = pd.DataFrame(df2_muni.Zip.str.split(' ',1).tolist(),\n",
    "                                   columns = ['Zip','Village'])\n",
    "\n",
    "mun_split = pd.DataFrame(df2_muni.Municipality.str.split(' ',1).tolist(),\n",
    "                                   columns = ['Mun. no.','Municipality'])\n",
    "\n",
    "# Merge data back together\n",
    "merge = pd.concat([zip_split, mun_split], axis=1, sort=False)\n",
    "mun_zip = merge[['Zip','Municipality']] \n",
    "\n",
    "# Construct new variable that only contain municpalities with zip code below 3000\n",
    "mun_zip['Int zip'] = mun_zip['Zip'].astype(int)\n",
    "our_sample = mun_zip[(mun_zip['Int zip'] < 3000)]\n",
    "\n",
    "# Drop duplicates so we have a simple list of the municipalities we are interested in\n",
    "municipalities = our_sample['Municipality'].drop_duplicates().reset_index()\n",
    "\n",
    "# List of our chosen municipalities\n",
    "municip = pd.DataFrame(municipalities['Municipality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table of school ranking\n",
    "url_school = 'https://www.sondagsavisen.dk/familien/2015-08-22-se-hele-listen-her-er-danmarks-bedste-og-vaerste-skole/'\n",
    "html = pd.read_html(url_school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean table of school ranking\n",
    "df_school = pd.DataFrame(html[0])\n",
    "head_school = df_school.rename(columns = df_school.iloc[0])\n",
    "school = head_school[1:]\n",
    "\n",
    "# Only include schools within our chosen municipalities\n",
    "schools = school[school['Kommune'].isin(municip['Municipality'])]\n",
    "n_school = schools.groupby(['Kommune']).size().reset_index(name='Count')\n",
    "\n",
    "\n",
    "# Find threshold for best schools. The numer of schools are chosen such that \n",
    "# we divide number of schools in a municipality with five to get the relative best schools\n",
    "thresh = [n_school['Kommune'], round(n_school['Count']//5)]\n",
    "threshold = pd.DataFrame(thresh).transpose()\n",
    "\n",
    "for i in range(0,len(threshold)):\n",
    "    if threshold['Count'][i]==0:\n",
    "        threshold['Count'][i]=1\n",
    "threshold\n",
    "\n",
    "# Merge threshold to our schools, so we can exclude schools with ranking above threshold\n",
    "schools_merge = pd.merge(schools, threshold, how='left',\n",
    "        left_on='Kommune', right_on='Kommune')\n",
    "\n",
    "schools_merge['Ranking'] = schools_merge['Placering Kommune'].astype(int)\n",
    "\n",
    "school_drop = schools_merge[(schools_merge.Ranking <= schools_merge.Count)].reset_index(drop = True)\n",
    "school_drop\n",
    "school_final = school_drop.drop(['Placering Kommune', 'UE 2014', 'Placering landsplan','Privat/ Offentlig','Count'], axis =1)\n",
    "\n",
    "# Rename schools that cannot be found using geocode\n",
    "new_name = []\n",
    "for i, row in school_final['Skolenavn'].iteritems():\n",
    "    if 'Østerhøjskolen' in row:\n",
    "        new_name.append('Østerhøj skole')\n",
    "    elif 'Kaptajn Johnsens Skole' in row:\n",
    "        new_name.append('Lykkesholms Alle 3A')\n",
    "    elif 'Sct. Joseph Søstrenes Skole S/I' in row:\n",
    "        new_name.append('Skovkrogen 19')\n",
    "    elif 'Atheneskolen – skolen for børn med særlige forudsætninger' in row:\n",
    "        new_name.append('Rosenkæret 22A')\n",
    "    elif 'Bagsværd Gymnasiums Grundskole' in row:\n",
    "        new_name.append('Aldershvilevej 138')\n",
    "    elif 'Greve Privatskole' in row:\n",
    "        new_name.append('Hundige Bygade 2')\n",
    "    elif 'Tjørnelyskolen' in row:\n",
    "        new_name.append('Lillevangsvej 48')\n",
    "    elif 'Skt. Pauls Skole' in row:\n",
    "        new_name.append('Sankt Pauls Skole')\n",
    "    elif 'Ådalens Privatskole' in row:\n",
    "        new_name.append('Skovvej 15')\n",
    "    elif 'Rungsted Private Realskole' in row:\n",
    "        new_name.append('Vallerød Banevej 23')\n",
    "    elif 'Hay Skolen' in row:\n",
    "        new_name.append('Sankt Hans Gade 25')\n",
    "    elif 'Amager’s International School' in row:\n",
    "        new_name.append('Engvej 141, 2300 København')\n",
    "    elif 'Jinnah International School' in row:\n",
    "        new_name.append('Skjulhøj Alle 59')\n",
    "    elif 'Iqra Privatskole' in row:\n",
    "        new_name.append('Hermodsgade 28')\n",
    "    elif 'Copenhagen Euro School' in row:\n",
    "        new_name.append('Gammel Kongevej 15')\n",
    "    elif 'Nørre Fælled Skole' in row:\n",
    "        new_name.append('Biskop Krags Vænge 7')\n",
    "    elif 'Al Hikma Skolen' in row:\n",
    "        new_name.append('Ellebjergvej 50')\n",
    "    elif 'Øresunds Internationale Skole' in row:\n",
    "        new_name.append('Engvej 153, 2300 København')\n",
    "    elif 'Sjællands Privatskole' in row:\n",
    "        new_name.append('Nattergalevej 32')\n",
    "    elif 'Baunehøjskolen' in row:\n",
    "        new_name.append('Baunegårdsvej')\n",
    "    elif 'Dronninggårdskolen' in row:\n",
    "        new_name.append('Rønnebærvej 33')\n",
    "    elif 'Uglegårdsskolen – Uglegård afdeling' in row:\n",
    "        new_name.append('Tingsryds Alle 25')\n",
    "    else:\n",
    "        new_name.append(row.split(',', 1)[0]) # split once, keep 1st part\n",
    "\n",
    "school_final.insert(loc=0, column='School name', value=new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Getting latitude and longitude for schools]\n",
    "# Import packages\n",
    "# import geopy.geocoders  # GeoPy - see https://pypi.org/project/geopy/\n",
    "# from geopy.geocoders import Nominatim # retrieve coordinates from addresses etc.\n",
    "geopy.geocoders.options.default_user_agent = 'my_app/1'\n",
    "geopy.geocoders.options.default_timeout = 15\n",
    "\n",
    "geolocator = Nominatim()\n",
    "# geolocator.headers  # check header\n",
    "# geolocator.timeout  # check time_out\n",
    "latitude = []\n",
    "longitude = []\n",
    "address = []\n",
    "\n",
    "for row in tqdm.tqdm(school_final['School name']):\n",
    "    row_string = str(row)\n",
    "    location = geolocator.geocode(row_string)\n",
    "    if isinstance(location, geopy.location.Location):\n",
    "        latitude.append(float(location.latitude))\n",
    "        longitude.append(float(location.longitude))\n",
    "    else:\n",
    "        print('Not found: ',row_string)\n",
    "        latitude.append(None)\n",
    "        longitude.append(None)\n",
    "school_final.insert(loc=0, column='Latitude', value=latitude)\n",
    "school_final.insert(loc=0, column='Longitude', value=longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distances to school from apartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# School coordinates\n",
    "school_coord = pd.DataFrame([school_final['Longitude'], school_final['Latitude']]).transpose()\n",
    "\n",
    "##############\n",
    "# FINAL DATASET FOR APARTMENTS\n",
    "################\n",
    "\n",
    "# Apartment coordinates\n",
    "data_apart = pd.read_csv('https://raw.githubusercontent.com/thornoe/sds_2018/master/CPH/Data/cph.csv')\n",
    "apartment_coord = pd.DataFrame([data_apart['Longitude'], data_apart['Latitude']]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distance between each school and apartment\n",
    "from geopy.distance import geodesic as dist\n",
    "\n",
    "school_distance = []\n",
    "for i in range(0,len(apartment_coord)):\n",
    "    for p in range(0,len(school_coord)):\n",
    "        apart_dist = (apartment_coord['Latitude'][i],apartment_coord['Longitude'][i])\n",
    "        school_dist = (school_coord['Latitude'][p],school_coord['Longitude'][p])\n",
    "        all_dist = dist(apart_dist,school_dist).km\n",
    "        school_distance.append(all_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct a function to split data so we have list divided over each apartment with distance to each school\n",
    "# i.e. split_dist[i] corresponds to apartment i \n",
    "\n",
    "def splitDataFrameIntoSmaller(df, chunkSize = 10000): \n",
    "    listOfDf = list()\n",
    "    numberChunks = len(df) // chunkSize + 1\n",
    "    for i in range(numberChunks):\n",
    "        listOfDf.append(df[i*chunkSize:(i+1)*chunkSize])\n",
    "    return listOfDf\n",
    "\n",
    "split_dist = splitDataFrameIntoSmaller(school_distance,len(school_coord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist_school = []\n",
    "for i in range(0,len(split_dist)-1):\n",
    "    minimum = min(split_dist[i])\n",
    "    min_dist_school.append(minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jail data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get jail data from github \n",
    "jail_data = pd.read_csv('https://raw.githubusercontent.com/thornoe/sds_2018/master/CPH/Data/jail.csv', sep = ';')/1000000\n",
    "\n",
    "# Calculate distance from jail to each apartment\n",
    "jail_distance = []\n",
    "for i in range(0,len(apartment_coord)):\n",
    "    for p in range(0,len(jail_data)):\n",
    "        apart_dist = (apartment_coord['Latitude'][i],apartment_coord['Longitude'][i])\n",
    "        jail_dist = (jail_data['Lat'][p],jail_data['Long'][p])\n",
    "        all_dist = dist(apart_dist,jail_dist).km\n",
    "        jail_distance.append(all_dist)\n",
    "\n",
    "# Split data\n",
    "split_jail = splitDataFrameIntoSmaller(jail_distance,len(jail_data)) \n",
    "\n",
    "# Calculate minimum distance to jail\n",
    "min_dist_jail = []\n",
    "for i in range(0,len(split_jail)-1):\n",
    "    minimum = min(split_jail[i])\n",
    "    min_dist_jail.append(minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metro data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get church data from github \n",
    "metro_data = pd.read_csv('https://raw.githubusercontent.com/thornoe/sds_2018/master/CPH/Data/metro.csv', sep = ';')/1000000\n",
    "\n",
    "# Calculate distance from jail to each apartment\n",
    "metro_distance = []\n",
    "for i in range(0,len(apartment_coord)):\n",
    "    for p in range(0,len(metro_data)):\n",
    "        apart_dist = (apartment_coord['Latitude'][i],apartment_coord['Longitude'][i])\n",
    "        metro_dist = (metro_data['lat'][p],metro_data['long'][p])\n",
    "        all_dist = dist(apart_dist,metro_dist).km\n",
    "        metro_distance.append(all_dist)\n",
    "\n",
    "# Split data\n",
    "split_metro = splitDataFrameIntoSmaller(metro_distance,len(metro_data)) \n",
    "\n",
    "# Calculate minimum distance to jail\n",
    "min_dist_metro = []\n",
    "for i in range(0,len(split_metro)-1):\n",
    "    minimum = min(split_metro[i])\n",
    "    min_dist_metro.append(minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance to cluster centrum\n",
    "centrum_coor = []\n",
    "for i in range(0,len(apartment_coord)):\n",
    "    apart_dist = (apartment_coord['Latitude'][i],apartment_coord['Longitude'][i])\n",
    "    cen_dist = (55.6841662,12.5657642)\n",
    "    centrum = dist(apart_dist,cen_dist).km\n",
    "    centrum_coor.append(centrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add minimum distance data to final data: \n",
    "data_apart['School_dist']=pd.DataFrame(min_dist_school)\n",
    "data_apart['Metro_dist']=pd.DataFrame(min_dist_metro)\n",
    "data_apart['Jail_dist']=pd.DataFrame(min_dist_jail)\n",
    "data_apart['Centrum_coor']=pd.DataFrame(centrum_coor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_apart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to GitHub\n",
    "data_apart.to_csv('C:/Users/thorn/OneDrive/Dokumenter/GitHub/sds_2018/CPH/Data/data_apar.csv', index=False)  # Save cleaned and merged data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Make table of descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut: Load cleaned and merged data from Github\n",
    "data_apart = pd.read_csv('https://raw.githubusercontent.com/thornoe/sds_2018/master/CPH/Data/data_apar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take subset of table for descriptive statistics. \n",
    "vars_ = ['Price', 'Owner_expense', 'Yearly_expenses', 'Sqm_price' , 'Sqm_owner_expenses', 'Sqm_yearly_expenses',\n",
    "         'Floor', 'Rooms', 'Area','Energy_saving', 'Days_on_market', 'School_dist', 'Metro_dist', 'Jail_dist']\n",
    "data_0 = data_apart.loc[:,vars_]\n",
    "data_1 = data_0.describe()\n",
    "# Round of decimal points and choose descriptives of interest. \n",
    "CPH_desc = data_1.astype(int)\n",
    "CPH_desc = CPH_desc.iloc[[1,2,3,5,7],:]\n",
    "descriptives = CPH_desc.transpose()\n",
    "print(descriptives)\n",
    "\n",
    "# Uncomment to get table in LaTeX format:\n",
    "# descriptives.to_latex()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Choropleth maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut: Load cleaned and merged data from Github\n",
    "data_apart = pd.read_csv('https://raw.githubusercontent.com/thornoe/sds_2018/master/CPH/Data/data_apar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of appartments for sale by municipality: \n",
    "mun_list = data_apart.groupby(['Municipality']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS**: For the choropleth map code you first need to download a folder of shape files from this link:\n",
    "\n",
    "LINK: https://github.com/thornoe/sds_2018/tree/master/CPH/Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data. \n",
    "KOM_0 = gpd.read_file(\"INSERT PATH TO DOWNLOAD .shp FILE!\")\n",
    "\n",
    "# Format the shape dataframe to only contain relevant stuff: \n",
    "KOM_1 = KOM_0[['KOMNAVN', 'REGIONNAVN','geometry']]\n",
    "KOM_2 = KOM_1[(Region_1.REGIONNAVN == 'Region Hovedstaden')].copy()\n",
    "shape = KOM_2.rename(columns={'KOMNAVN':'Municipality'})\n",
    "\n",
    "# Drop Bornholm and Christiansø from map\n",
    "shape.drop(shape[shape.Municipality =='Bornholm'].index, inplace=True)\n",
    "shape.drop(shape[shape.Municipality =='Christiansø'].index, inplace=True)\n",
    "\n",
    "# Choose the subsample of the dataframe you want to plot on the Choropleth:  \n",
    "grouped = data_apart.groupby(['Municipality'], as_index=False).mean()\n",
    "map_data = grouped[['Municipality','Sqm_price', 'log_sqm_price', 'Owner_expense']]\n",
    "\n",
    "# Remove municipalities without listings from map\n",
    "shape_1 = shape[(shape.Municipality != 'Frederikssund')].copy()\n",
    "shape_2 = shape_1[(shape.Municipality != 'Helsingør')].copy()\n",
    "shape_3 = shape_2[(shape.Municipality != 'Gribskov')].copy()\n",
    "shape_4 = shape_3[(shape.Municipality != 'Halsnæs')].copy()\n",
    "shape_5 = shape_4[(shape.Municipality != 'Hillerød')].copy()\n",
    "\n",
    "# join the geodataframe with the cleaned up csv dataframe\n",
    "merged = shape_5.set_index('Municipality').join(map_data.set_index('Municipality'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOROPLETH WITH SQUARE-METER-PRICE \n",
    "# set a variable that will call what column we want to visualise on the map\n",
    "variable = 'Sqm_price'\n",
    "\n",
    "# set the range for the choropleth\n",
    "vmin, vmax = 14000, 60000\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig1, ax1 = plt.subplots(1, figsize=(10, 6))\n",
    "\n",
    "# create map\n",
    "merged.plot(column=variable, cmap='BuPu', linewidth=0.8, ax=ax1, edgecolor='0.6')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Create colorbar as a legend\n",
    "sm = plt.cm.ScalarMappable(cmap='BuPu', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "\n",
    "# add the colorbar to the figure\n",
    "cbar = fig1.colorbar(sm)\n",
    "\n",
    "# uncomment to save figure\n",
    "#fig1.savefig('Sqm_mean.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to get at look at mean of sqm price on municipalities.\n",
    "#grouped[['Municipality', 'Sqm_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for listings CHOROPLETH\n",
    "Listings = data_apart.groupby(['Municipality']).size().reset_index(name='counts')\n",
    "merged_listings = shape_5.set_index('Municipality').join(Listings.set_index('Municipality'))\n",
    "\n",
    "# CHOROPLETH WITH NUMBER OF LISTINGS\n",
    "\n",
    "# set a variable that will call what column we want to visualise on the map\n",
    "variable = 'counts'\n",
    "\n",
    "# set the range for the choropleth\n",
    "vmin, vmax = 0, 1536\n",
    "\n",
    "# create figure and axes for Matplotlib\n",
    "fig3, ax3 = plt.subplots(1, figsize=(10, 6))\n",
    "\n",
    "# create map\n",
    "merged_listings.plot(column=variable, cmap='OrRd', linewidth=0.8, ax=ax3, edgecolor='0.5')\n",
    "ax3.axis('off')\n",
    "\n",
    "# Create colorbar as a legend\n",
    "sm = plt.cm.ScalarMappable(cmap='OrRd', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "\n",
    "# empty array for the data range\n",
    "sm._A = []\n",
    "\n",
    "# add the colorbar to the figure\n",
    "cbar = fig3.colorbar(sm)\n",
    "\n",
    "#fig3.savefig('Listings_1.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Making correlation matrices to get an overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut: Load cleaned and merged data from Github\n",
    "data_apart = pd.read_csv('https://raw.githubusercontent.com/thornoe/sds_2018/master/CPH/Data/data_apar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set and create subset to perform Machine Learning on\n",
    "corr_set = data_apart[['log_sqm_price', 'Floor', 'Land_area','Rooms', 'Area', 'Owner_expense', 'Energy_saving', \n",
    "                     'School_dist', 'Metro_dist', 'Jail_dist', 'Centrum_coor']]\n",
    "\n",
    "corr_set.columns = ['log sqm price', 'floor', 'Land_area', 'rooms', 'area (m2)', 'owner expense',\n",
    "                 'energy rating', 'distance to school', 'distance to metro', 'distance to jail', 'distance to centrum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the Correlation matrix\n",
    "cols = ['log sqm price','distance to school', 'distance to metro', 'distance to jail', 'distance to centrum']\n",
    "fig4, hm = plt.subplots(figsize=(10,10))\n",
    "cm = np.corrcoef(ML_set[cols].values.T)\n",
    "sns.set(font_scale=1.5)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 25}, yticklabels=cols, xticklabels=cols)\n",
    "# fig4.savefig('corr1.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the Correlation matrix\n",
    "cols = ['log sqm price', 'owner expense', 'floor', 'area (m2)', 'rooms', 'energy rating']\n",
    "fig5, hm2 = plt.subplots(figsize=(10,10))\n",
    "cm = np.corrcoef(ML_set[cols].values.T)\n",
    "sns.set(font_scale=1.5)\n",
    "hm2 = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 25}, yticklabels=cols, xticklabels=cols)\n",
    "# fig5.savefig('corr2.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data and define functions for performing K-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances  # for K means\n",
    "cph = pd.read_csv('https://raw.githubusercontent.com/thornoe/sds_2018/master/CPH/Data/data_apar.csv')\n",
    "seed = 2900\n",
    "\n",
    "##############################################################################\n",
    "#         Coding functions for the the K-means Clustering algorithm          #\n",
    "##############################################################################\n",
    "def initialize_clusters(X, k, seed):\n",
    "    \"\"\"Initialization: first Expectation of the cluster centroids (centers)\"\"\"\n",
    "    random.seed(seed)\n",
    "    idx = random.sample(range(len(X)), k)\n",
    "    centroids = X[idx]\n",
    "    return centroids\n",
    "\n",
    "# Maximization step - assign each datapoint to the closests centroids\n",
    "def maximize(X, centroids):\n",
    "    \"\"\"Calculates the distance from each point to each centroid (cluster center)\n",
    "    Next runs an argmin operation on the matrix to obtain the cluster_assignment,\n",
    "    assigning each datapoint to the closest centroid.\"\"\"\n",
    "    dist_matrix = pairwise_distances(centroids, X)\n",
    "    cluster_assignment = dist_matrix.T.argsort(axis=1)[:,0]\n",
    "    return cluster_assignment\n",
    "\n",
    "# Updating expectations step\n",
    "def update_expectation(X, k, cluster_assignment):\n",
    "    \"\"\"Update Expectation of the cluster centroids by applying the .mean function\n",
    "    on the subset of the data that is assigned to each cluster\"\"\"\n",
    "    new_centroids = np.zeros((k,len(X[0])))\n",
    "    for i in range(k):\n",
    "        subset = X[cluster_assignment==i]  # filter the data with a boolean vector\n",
    "        new_centroids[i] = subset.mean(axis=0)  # calculate the mean on the subset (axis=0 is on the columns)\n",
    "    return new_centroids\n",
    "\n",
    "# Convergence\n",
    "def fit_transform(X, k, max_iter, seed):\n",
    "    old_centroids = initialize_clusters(X, k, seed)\n",
    "    iterations = 0\n",
    "    for i in np.arange(max_iter):\n",
    "        cluster_assignment = maximize(X, old_centroids)\n",
    "        new_centroids = update_expectation(X, k, cluster_assignment)\n",
    "        if np.array_equal(old_centroids, new_centroids):\n",
    "            break\n",
    "        else:\n",
    "            old_centroids = new_centroids\n",
    "            iterations = iterations + 1\n",
    "    print('Centroids:', '\\n', old_centroids,\n",
    "        '\\nIterations:', '\\n', iterations,)\n",
    "    return cluster_assignment, old_centroids, iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering using apartment prices per square meter and coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "#            Clusters in Greater Copenhagen - 3-dimensional              #\n",
    "##########################################################################\n",
    "X = cph.loc[:,['Latitude', 'Longitude', 'log_sqm_price']].values  # Define a matrix using the .values method\n",
    "\n",
    "max_iter = 100  # maximum number of iterations\n",
    "k = 3  # number of clusters\n",
    "cluster_assignment, centroids, iterations = fit_transform(X, k, max_iter, seed)  # Set the number of clusters\n",
    "\n",
    "XD = cph.reindex(columns = ['Latitude', 'Longitude', 'log_sqm_price', 'Sqm_price'])\n",
    "XD.insert(loc=3, column='Cluster', value=cluster_assignment)\n",
    "XD['Cluster'] = XD['Cluster'].astype('category')\n",
    "# XD['Cluster'].dtype\n",
    "XD['Cluster'] = XD['Cluster'].cat.rename_categories(['High', 'Low', 'Middle'])  # ({'0': 'High', '1':'Low', '2':'Middle'})\n",
    "XD['Cluster'].cat.categories\n",
    "\n",
    "centroids = pd.DataFrame(centroids)\n",
    "centroids.columns = ['Latitude', 'Longitude', 'log_sqm_price']\n",
    "cluster = ['High', 'Low', 'Middle']\n",
    "centroids.insert(loc=3, column='Cluster', value=cluster)\n",
    "\n",
    "# Plot\n",
    "sns.set(style='ticks')\n",
    "fig, ax = plt.subplots(figsize = (15, 15*0.87))\n",
    "ax1 = sns.scatterplot(x='Longitude', y='Latitude', hue='Cluster', hue_order=['Low', 'Middle', 'High'],\n",
    "    size = 'log_sqm_price', sizes=(1,300), palette=\"Paired\", legend='brief', data=XD, alpha=0.4)\n",
    "ax2 = sns.scatterplot(x='Longitude', y='Latitude', hue='Cluster', hue_order=['Low', 'Middle', 'High'],\n",
    "    size = 'log_sqm_price', sizes=(800,1200), palette=\"Paired\", legend=False, data=centroids, marker='X', alpha=0.9)\n",
    "ax = ax1, ax2\n",
    "# fig.savefig(\"CPH/Fig/cluster.pdf\", dpi=600, bbox_inches='tight')\n",
    "\n",
    "\n",
    "print('Share in each cluster:', '\\n', XD['Cluster'].value_counts(normalize=True))\n",
    "\n",
    "# Descriptive statistics\n",
    "data_0 = XD['Sqm_price'].loc[XD['Cluster'] == 'Low']\n",
    "data_1 = data_0.describe([.025, .5, .975])\n",
    "data_0 = XD['Sqm_price'].loc[XD['Cluster'] == 'Middle']\n",
    "data_2 = data_0.describe([.025, .5, .975])\n",
    "data_0 = XD['Sqm_price'].loc[XD['Cluster'] == 'High']\n",
    "data_3 = data_0.describe([.025, .5, .975])\n",
    "CPH_desc = pd.DataFrame(data_1)\n",
    "CPH_desc.columns = ['Low']\n",
    "CPH_desc.insert(loc=1, column='Middle', value=data_2)\n",
    "CPH_desc.insert(loc=2, column='High', value=data_3)\n",
    "CPH_desc = CPH_desc.astype(int)\n",
    "descriptives = CPH_desc.transpose()\n",
    "print('Descriptive statistics for each cluster:', '\\n', CPH_desc)\n",
    "Perform K-means clustering using total yearly expenses per square meter and coordinates.# descriptives.to_latex()  # print descriptives to latex: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_latex.html\n",
    "\n",
    "### Number of iterations for different seed numbers\n",
    "# no_iterations = []\n",
    "# for seed in range(0, 500):\n",
    "#     cluster_assignment, centroids, iterations = fit_transform(X, k, max_iter, seed)  # Set the number of clusters\n",
    "#     no_iterations.append(iterations)\n",
    "# np.unique(no_iterations, return_counts=False)\n",
    "\n",
    "### Map dimensions\n",
    "# latitude_span = 55.94 - 55.522\n",
    "# longitude_span = 12.67 - 12.19\n",
    "# print(latitude_span / longitude_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering using total yearly expenses per square meter and coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = cph.loc[:,['Latitude', 'Longitude', 'log_sqm_yearly_exp']].values  # Define a matrix using the .values method\n",
    "seed = 2900\n",
    "max_iter = 100  # maximum number of iterations\n",
    "k = 3  # number of clusters\n",
    "cluster_assignment, centroids, iterations = fit_transform(Z, k, max_iter, seed)  # Set the number of clusters\n",
    "\n",
    "ZD = cph.reindex(columns = ['Latitude', 'Longitude', 'log_sqm_yearly_exp', 'Yearly_expenses'])\n",
    "ZD.insert(loc=3, column='Cluster', value=cluster_assignment)\n",
    "ZD['Cluster'] = ZD['Cluster'].astype('category')\n",
    "# ZD['Cluster'].dtype\n",
    "ZD['Cluster'] = ZD['Cluster'].cat.rename_categories(['High', 'Middle', 'Low'])  # ({'0': 'High', '1':'Middle', '2':'Low'})\n",
    "ZD['Cluster'].cat.categories\n",
    "\n",
    "centroids = pd.DataFrame(centroids)\n",
    "centroids.columns = ['Latitude', 'Longitude', 'log_sqm_yearly_exp']\n",
    "cluster = ['High', 'Middle', 'Low']\n",
    "centroids.insert(loc=3, column='Cluster', value=cluster)\n",
    "\n",
    "# Plot\n",
    "sns.set(style='ticks')\n",
    "fig, ax = plt.subplots(figsize = (15, 15*0.87))\n",
    "ax1 = sns.scatterplot(x='Longitude', y='Latitude', hue='Cluster', hue_order=['Low', 'Middle', 'High'],\n",
    "    size = 'log_sqm_yearly_exp', sizes=(1,300), palette=\"Paired\", legend='brief', data=ZD, alpha=0.4)\n",
    "ax2 = sns.scatterplot(x='Longitude', y='Latitude', hue='Cluster', hue_order=['Low', 'Middle', 'High'],\n",
    "    size = 'log_sqm_yearly_exp', sizes=(800,1200), palette=\"Paired\", legend=False, data=centroids, marker='X', alpha=0.9)\n",
    "ax = ax1, ax2\n",
    "# fig.savefig(\"CPH/Fig/cluster_yearlyexpenses.pdf\", dpi=600, bbox_inches='tight')\n",
    "\n",
    "ZD['Differences'] = np.where(XD['Cluster'] != ZD['Cluster'], ZD['Cluster'], np.nan)\n",
    "\n",
    "print('Cluster distribution using apartment price per square meter:', '\\n',\n",
    "    XD['Cluster'].value_counts(normalize=True),\n",
    "    '\\nCluster distribution using total yearly expenses per square meter:', '\\n',\n",
    "    ZD['Cluster'].value_counts(normalize=True),\n",
    "    \"\\nNew 'entrances' in each cluster:\", '\\n',\n",
    "    ZD['Differences'].value_counts(normalize=False),\n",
    "    '\\nShare that change cluster:', '\\n',\n",
    "    (311+109+32)/len(ZD) ) # 11.4 pct. belongs to a different cluster\n",
    "\n",
    "# ZD['Yearly_expenses'].loc[ZD['Cluster'] == 'Low'].describe([.025, .5, .975])\n",
    "# ZD['Yearly_expenses'].loc[ZD['Cluster'] == 'Middle'].describe([.025, .5, .975])\n",
    "# ZD['Yearly_expenses'].loc[ZD['Cluster'] == 'High'].describe([.025, .5, .975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shortcut: Load cleaned and merged data from Github\n",
    "data_apart = pd.read_csv('https://raw.githubusercontent.com/thornoe/sds_2018/master/CPH/Data/no_extremes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Preparing the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages for Machine Learning\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set and create subset to perform Machine Learning on\n",
    "ML_set = data_apart[['log_sqm_price', 'Municipality', 'Floor', 'Land_area','Rooms', 'Area', 'Owner_expense', 'Energy_saving', \n",
    "                     'School_dist', 'Metro_dist', 'Jail_dist', 'Centrum_coor', 'Days_on_market']]\n",
    "ML_dummy = pd.get_dummies(ML_set, columns=['Municipality'])\n",
    "\n",
    "X = ML_dummy.iloc[:,1:]\n",
    "y = ML_set[['log_sqm_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into development (2/3) and test data (1/3)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=1)\n",
    "# splitting development into train (1/3) and validation (1/3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=1/2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Set polynomial features = 3 and create pipelines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#        LINEAR           #\n",
    "###########################\n",
    "pipe_lr = make_pipeline(PolynomialFeatures(degree = 3,include_bias=False), \n",
    "                            StandardScaler(),\n",
    "                            LinearRegression())\n",
    "print('Linear: done')\n",
    "###########################\n",
    "#        Lasso            #\n",
    "########################### \n",
    "pipe_lasso = make_pipeline(PolynomialFeatures(degree=3, include_bias=False), \n",
    "                                  StandardScaler(),\n",
    "                                  Lasso())\n",
    "print('Lasso: done')\n",
    "###########################\n",
    "#        LASSO CV         #\n",
    "###########################\n",
    "lambdas = np.logspace(-4,4, 12)\n",
    "kfolds = KFold(n_splits=10)\n",
    "RMSE_lassoCV = []\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    print('This was lambda= ' + str(lambda_))\n",
    "    pipe_lassoCV = make_pipeline(PolynomialFeatures(degree=3,include_bias=False), \n",
    "                                  StandardScaler(),\n",
    "                                  Lasso(alpha=lambda_, random_state=1))    \n",
    "    RMSE_lassoCV_ = []\n",
    "    \n",
    "    for train_idx, val_idx in kfolds.split(X_dev, y_dev):\n",
    "        \n",
    "        X_train, y_train, = X_dev.iloc[train_idx], y_dev.iloc[train_idx]\n",
    "        X_val, y_val = X_dev.iloc[val_idx], y_dev.iloc[val_idx] \n",
    "\n",
    "        pipe_lassoCV.fit(X_train, y_train)\n",
    "        RMSE_lassoCV_.append(mse(y_val, pipe_lassoCV.predict(X_val))**(1/2)) \n",
    "    RMSE_lassoCV.append(RMSE_lassoCV_)\n",
    "\n",
    "optimalCV = pd.DataFrame(RMSE_lassoCV, index=lambdas).mean(axis=1).nsmallest(1)\n",
    "print(optimalCV) # This prints optimal lambda and RMSE. \n",
    "\n",
    "# Fit training data with optimal lambda\n",
    "pipe_lassoCV = make_pipeline(PolynomialFeatures(degree=3, include_bias=False), \n",
    "                                StandardScaler(),\n",
    "                                Lasso(alpha=optimalCV.index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Fit on development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.fit(X_dev, y_dev)\n",
    "pipe_lasso.fit(X_dev,y_dev)\n",
    "pipe_lassoCV.fit(X_dev,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Get MAE and RMSE from test data and make table of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model\n",
    "MAE_lr = mae(y_test, pipe_lr.predict(X_test))\n",
    "RMSE_lr = mse(y_test, pipe_lr.predict(X_test))**(1/2)\n",
    "\n",
    "# Lasso model\n",
    "MAE_lasso = mae(y_test, pipe_lasso.predict(X_test))\n",
    "RMSE_lasso = mse(y_test, pipe_lasso.predict(X_test))**(1/2)\n",
    "\n",
    "# Lasso CV\n",
    "MAE_lasso_CV = mae(y_test, pipe_lassoCV.predict(X_test))\n",
    "RMSE_lasso_CV = mse(y_test, pipe_lassoCV.predict(X_test))**(1/2)\n",
    "\n",
    "# Generate table of results\n",
    "MAE = [MAE_lr, MAE_lasso, MAE_lasso_CV]\n",
    "RMSE = [RMSE_lr, RMSE_lasso, RMSE_lasso_CV]\n",
    "\n",
    "Results1 = pd.DataFrame({'MAE': MAE, 'RMSE': RMSE}, index=('Linear', 'Lasso', 'Lasso CV'))\n",
    "Results1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Optimize on polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Create pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#        LINEAR           #\n",
    "###########################\n",
    "pol = range(1,5)\n",
    "perform_lr = []\n",
    "\n",
    "# First loop over polynomial degrees to find best performance for linear\n",
    "for dg in pol:\n",
    "    pipe_lr = make_pipeline(PolynomialFeatures(degree = dg,include_bias=False), \n",
    "                           StandardScaler(),\n",
    "                           LinearRegression())  \n",
    "    print('Done with: '+str(dg))\n",
    "    # Fit the training data\n",
    "    pipe_lr.fit(X_train, y_train)\n",
    "    perform_lr.append(mse(y_val, pipe_lr.predict(X_val))**(1/2))\n",
    "\n",
    "optimal_pol_lr = pd.Series(perform_lr,index=pol).nsmallest(1)\n",
    "# Define pipeline for linear\n",
    "pipe_lr = make_pipeline(PolynomialFeatures(degree = optimal_pol_lr.index[0],include_bias=False), \n",
    "                           StandardScaler(),\n",
    "                           LinearRegression())\n",
    "\n",
    "print('Linear: ' + str(optimal_pol_lr.index[0]))\n",
    "\n",
    "###########################\n",
    "#        Lasso            #\n",
    "###########################\n",
    "\n",
    "perform_lasso = []\n",
    "\n",
    "# First loop over polynomial degrees to find best performance\n",
    "for dg in pol:\n",
    "    pipe_lasso = make_pipeline(PolynomialFeatures(degree = dg,include_bias=False), \n",
    "                               StandardScaler(),\n",
    "                               Lasso()) \n",
    "    print('Done with: '+str(dg))\n",
    "    # Fit the training data\n",
    "    pipe_lasso.fit(X_train, y_train)\n",
    "    perform_lasso.append(mse(y_val, pipe_lasso.predict(X_val))**(1/2))\n",
    "optimal_pol_lasso = pd.Series(perform_lasso,index=pol).nsmallest(1)\n",
    "# Define pipeline for lasso\n",
    "pipe_lasso = make_pipeline(PolynomialFeatures(degree=optimal_pol_lasso.index[0], include_bias=False), \n",
    "                              StandardScaler(),\n",
    "                              Lasso())\n",
    "\n",
    "print('Lasso: ' + str(optimal_pol_lasso.index[0]))\n",
    "\n",
    "###########################\n",
    "#        LASSO CV         #\n",
    "###########################\n",
    "\n",
    "kfolds = KFold(n_splits=10)\n",
    "RMSE_lassoCV = []\n",
    "\n",
    "for dg in pol:\n",
    "    \n",
    "    pipe_lassoCV = make_pipeline(PolynomialFeatures(degree=dg,include_bias=False), \n",
    "                                  StandardScaler(),\n",
    "                                  Lasso(alpha=optimalCV.index[0], random_state=1)) \n",
    "    print('Done with: '+str(dg))\n",
    "    RMSE_lassoCV_ = []\n",
    "    \n",
    "    for train_idx, val_idx in kfolds.split(X_dev, y_dev):\n",
    "        \n",
    "        X_train, y_train, = X_dev.iloc[train_idx], y_dev.iloc[train_idx]\n",
    "        X_val, y_val = X_dev.iloc[val_idx], y_dev.iloc[val_idx] \n",
    "\n",
    "        pipe_lassoCV.fit(X_train, y_train)\n",
    "        RMSE_lassoCV_.append(mse(y_val, pipe_lassoCV.predict(X_val))**(1/2))    \n",
    "    RMSE_lassoCV.append(RMSE_lassoCV_)\n",
    "\n",
    "optimal_pol_LCV = pd.DataFrame(RMSE_lassoCV, index=pol).mean(axis=1).nsmallest(1)\n",
    "print(optimal_pol_LCV) # This prints optimal dg and RMSE. \n",
    "\n",
    "# Lasso CV pipeline\n",
    "pipe_lassoCV = make_pipeline(PolynomialFeatures(degree=optimal_pol_LCV.index[0], include_bias=False), \n",
    "                                StandardScaler(),\n",
    "                                Lasso(alpha=optimalCV.index[0]))\n",
    "\n",
    "print('Lasso CV: ' + str(optimal_pol_LCV.index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Fit on development data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr.fit(X_dev, y_dev)\n",
    "pipe_lasso.fit(X_dev,y_dev)\n",
    "pipe_lassoCV.fit(X_dev,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Get MAE and RMSE from test data and make table of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model\n",
    "MAE_lr = mae(y_test, pipe_lr.predict(X_test))\n",
    "RMSE_lr = mse(y_test, pipe_lr.predict(X_test))**(1/2)\n",
    "\n",
    "# Lasso model\n",
    "MAE_lasso = mae(y_test, pipe_lasso.predict(X_test))\n",
    "RMSE_lasso = mse(y_test, pipe_lasso.predict(X_test))**(1/2)\n",
    "\n",
    "# Lasso CV\n",
    "MAE_lasso_CV = mae(y_test, pipe_lassoCV.predict(X_test))\n",
    "RMSE_lasso_CV = mse(y_test, pipe_lassoCV.predict(X_test))**(1/2)\n",
    "\n",
    "# Generate table of results\n",
    "MAE = [MAE_lr, MAE_lasso, MAE_lasso_CV]\n",
    "RMSE = [RMSE_lr, RMSE_lasso, RMSE_lasso_CV]\n",
    "\n",
    "Results2 = pd.DataFrame({'MAE': MAE, 'RMSE': RMSE}, index=('Linear', 'Lasso', 'Lasso CV'))\n",
    "Results2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
